{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import nltk\n",
    "import random\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import collections\n",
    "import brisk\n",
    "from numba import jit\n",
    "from scipy.spatial.distance import pdist\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-164c74e97f89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Load test set and candidate set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mwe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m'/pre-processing.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mwe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'similar_words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'similar_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meval_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3826\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m         \"\"\"\n\u001b[1;32m-> 3828\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3829\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\py36\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1300\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-164c74e97f89>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Load test set and candidate set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mwe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;34m'/pre-processing.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mwe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'similar_words'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'similar_words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meval_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_topics = 120  # topic number\n",
    "rng = np.random.mtrand._rand\n",
    "rands = rng.rand(1024 ** 2 // 8) # rand seeds\n",
    "\n",
    "# Load the model\n",
    "path_dir = './dataset/'\n",
    "datasets = 'nips'\n",
    "\n",
    "with open(path_dir + datasets + '/gibbs_lda_model.pickle','rb') as file:\n",
    "    model = pickle.loads(file.read())\n",
    "\n",
    "# Load the vocab\n",
    "path = path_dir + datasets + '/data/'\n",
    "vocab = lda.datasets.load_reuters_vocab(path)\n",
    "voc = collections.defaultdict()\n",
    "for index, i in enumerate(vocab):\n",
    "    word = i.replace('\\n', '')\n",
    "    if not word:\n",
    "        continue\n",
    "    voc[word] = index\n",
    "\n",
    "# Load test set and candidate set\n",
    "we = pd.read_csv(path_dir + datasets +  '/pre-processing.csv')\n",
    "we['similar_words'] = we['similar_words'].map(lambda x: eval(x))\n",
    "\n",
    "def eval_string(x):\n",
    "    x = x.replace('[','').replace(']','').replace(',','')\n",
    "    x = x.strip().split()\n",
    "    x = [float(i) for i in x]\n",
    "    return x\n",
    "we['word_embedding'] = we['word_embedding'].map(lambda x:eval_string(x))\n",
    "\n",
    "def evals(x):\n",
    "    if x == '<class \\'list\\'>':\n",
    "        return []\n",
    "    else:\n",
    "        return eval(x)\n",
    "we['similar_mixed'] = we['similar_mixed'].map(lambda x: evals(x))\n",
    "\n",
    "f_s = open(path_dir + datasets + '/submissions.csv')\n",
    "submissions = pd.read_csv(f_s)\n",
    "submissions['paper_text_tokens'] = submissions['paper_text_tokens'].map(lambda x: eval(x))\n",
    "\n",
    "# Load word embedding (it will cost a lot of time)\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(path_dir + 'wikien.vec')\n",
    "\n",
    "# Load Bert\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True,parallel=True)\n",
    "def infer(nzw, x_index, n_iter=500, m_alpha=0.1, m_eta=0.01):\n",
    "    '''\n",
    "    infer the new documents \n",
    "    ----------\n",
    "    Input\n",
    "        nzw: the results of LDA model after training stage\n",
    "        x_index: array-like, shape(, n_features)\n",
    "            Testing vector, where n_features is the number of features.\n",
    "        n_iter: iterations, default 500\n",
    "        m_alpha: alpha, gibbs LDA parameter, default 0.1\n",
    "        m_eta: eta, gibbs LDA parameter, default 0.01\n",
    "    Output\n",
    "        Topic distribution of the test document \n",
    "    '''\n",
    "    global n_topics\n",
    "    n_rand = 131072  # a large number\n",
    "    alpha = np.repeat(m_alpha, n_topics).astype(np.float64)\n",
    "    eta = np.repeat(m_eta, len(voc)).astype(np.float64)\n",
    "    eta_sum = np.sum(eta) \n",
    "\n",
    "    # Initialize\n",
    "    nz = np.zeros(n_topics)\n",
    "    ZS = np.empty(len(x_index), np.int32)\n",
    "    dist_sum = np.zeros(n_topics)\n",
    "    for index, w in enumerate(x_index):\n",
    "        z_new = index % n_topics\n",
    "        ZS[index] = z_new\n",
    "        nzw[z_new, w] += 1\n",
    "        nz[z_new] += 1\n",
    "    # Sampling\n",
    "    for it in range(n_iter):\n",
    "        for index, w in enumerate(x_index):\n",
    "            z = ZS[index]\n",
    "            nzw[z, w] -= 1\n",
    "            nz[z] -= 1\n",
    "            dist_cum = 0\n",
    "            for k in range(n_topics):\n",
    "                dist_cum += (nzw[k, w] + eta[w]) / (nz[k] + eta_sum) * (nz[k] + alpha[k])\n",
    "                dist_sum[k] = dist_cum\n",
    "\n",
    "            r = rands[index % n_rand] * dist_cum \n",
    "            z_new = brisk.bisect_left(dist_sum, r)\n",
    "            ZS[index] = z_new\n",
    "            nzw[z_new, w] += 1\n",
    "            nz[z_new] += 1\n",
    "\n",
    "    return nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff(word_count, voc, Nkv, target):\n",
    "    diff = 0\n",
    "    for (word, count) in word_count.items():\n",
    "        wi = voc[word]\n",
    "        diff += count * (Nkv[:, wi][target]) / brisk.sum(Nkv[:, wi])\n",
    "\n",
    "    return diff\n",
    "\n",
    "def rank_cal(x, target, result, sim_w, sim_s, flag):\n",
    "    new_x = x / sum(x)\n",
    "    output = {}\n",
    "    for index, k in enumerate(new_x):\n",
    "        output[index] = k\n",
    "\n",
    "    output = sorted(output.items(), key = lambda x:x[1], reverse = True)\n",
    "    for index, k in enumerate(output):\n",
    "        if k[0] != target and k[1] == 0:\n",
    "            result[flag].append([index + 1, k[0], k[1], sim_w, sim_s])\n",
    "            break\n",
    "        if k[0] == target:\n",
    "            result[flag].append([index + 1, k[0], k[1], sim_w, sim_s])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech screening, see http://www.nltk.org/_modules/nltk/tag.html for more details.\n",
    "wordq = [['JJ', 'JJR', 'JJS'], ['NN', 'NNS', 'NNP', 'NNPS'], \n",
    "         ['PRP', 'PRP$'], ['RB', 'RBR', 'RBS'],\n",
    "         ['VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n",
    "        \n",
    "def pos_filter(x, word):\n",
    "    if x[0][0] == word:\n",
    "        del x[0]\n",
    "    pos_list = nltk.pos_tag(np.array(x)[:, 0].tolist())\n",
    "    pos_list = np.array(pos_list)[:, 1]\n",
    "    \n",
    "    target = nltk.pos_tag([word])\n",
    "    target_list = []\n",
    "    for i in wordq:\n",
    "        if target[0][1] in i:\n",
    "            target_list = i\n",
    "\n",
    "    index = [k for k in range(len(np.array(pos_list))) if (pos_list[k] in target_list)]\n",
    "    res = [tuple(p) for p in np.array(x)[index]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word_index, word, Nkv, new):\n",
    "    em_word_index = int(list(new[new['word'].isin([word])].index.values)[0])\n",
    "    ori_phi = Nkv[:, word_index] / sum(Nkv[:, word_index])\n",
    "    similar_words = new.loc[em_word_index, 'similar_mixed']\n",
    "    similar_words = pos_filter(similar_words, word)\n",
    "    return similar_words, ori_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 5 words before and after the current word to form a sentence\n",
    "def merge_sentence(pos, doc, word_candidate):\n",
    "    if pos >= 5 and pos <= len(doc) - 5:\n",
    "        ori_sentence = ' '.join(doc[pos - 5: pos + 5])  # combine sentence\n",
    "        adv_sentence = ' '.join(doc[pos - 5: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: pos + 5])\n",
    "    elif pos < 5:\n",
    "        ori_sentence = ' '.join(doc[:10])\n",
    "        adv_sentence = ' '.join(doc[0: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: 10])\n",
    "    else:\n",
    "        ori_sentence = ' '.join(doc[-10:])\n",
    "        adv_sentence = ' '.join(doc[-10: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1:])\n",
    "    \n",
    "    return ori_sentence, adv_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seak_for_word(doc, target, nd, Nkv, length, Att_algo, t, alpha = 0.005):\n",
    "    '''\n",
    "    Input\n",
    "        doc: the victim submission\n",
    "        target: the target topic\n",
    "        nd: the predicted topic sampling count of the victim submission\n",
    "        Nkv: the training topic-word sampling count of whole voc\n",
    "        length: the length of original victim submission\n",
    "        Att_algo: attack algorithm, different level parameter\n",
    "        t: the unique words substitution parameter\n",
    "        alpha: modification budget, default 0.005\n",
    "    Output\n",
    "        attack path, adversarial topic sampling count estimation, adversarial submission\n",
    "    '''\n",
    "    global voc\n",
    "    global path\n",
    "    solution_space = []  # Get solution space\n",
    "    for i in sub_sample:\n",
    "        if i in we['word'].values and i not in solution_space:\n",
    "            solution_space.append(i)\n",
    "\n",
    "    new_we = we[we['word'].isin(solution_space)]\n",
    "    new = copy.deepcopy(new_we)\n",
    "    iterations = int(length * alpha)\n",
    "    nd = nd.astype('float64')\n",
    "    word_dis, sentence_dis = 0.0, 0.0  # word distortion, sentence distortion\n",
    "    adv_solu = {}  # attack path\n",
    "    already = []\n",
    "    vic_visited = []\n",
    "    word_counts = collections.Counter(doc)  # word counts\n",
    "    d_v = 0.4  # word embedding distance threshold\n",
    "    cur_change = 0  # current modification count\n",
    "\n",
    "    word_contribution = collections.defaultdict(list)  # word-phi dict\n",
    "    contributions = 0.0\n",
    "    for index, word in enumerate(list(set(doc))):\n",
    "        word_index = voc[word]\n",
    "        phi = Nkv[:, word_index] / sum(Nkv[:, word_index])\n",
    "        word_contribution[word_index] = phi[target]\n",
    "        contributions += (word_contribution[word_index] * word_counts[word])\n",
    "\n",
    "    if 'baseline' in Att_algo:\n",
    "        if Att_algo != 'baseline_4':\n",
    "            victims = random.sample(range(len(doc)), iterations)\n",
    "        if Att_algo == 'baseline_3' or Att_algo == 'baseline_4':\n",
    "            attack_words = (-model.topic_word_[target]).argsort()[:]\n",
    "            base3iter = 0\n",
    "            base4iter = 0\n",
    "            att_words = []\n",
    "        for i in range(iterations):\n",
    "            if Att_algo == 'baseline_4':\n",
    "                base4flag = 0\n",
    "                while base4flag == 0:\n",
    "                    target_index = attack_words[base4iter]\n",
    "                    sub_word = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "                    if sub_word in att_words:\n",
    "                        base4iter += 1\n",
    "                        continue\n",
    "                    # get the most similar words in doc\n",
    "                    sw_list = word_embedding_model.most_similar_to_given_top(sub_word, list(set(doc)), topk=len(list(set(doc))))\n",
    "                    # pos filter\n",
    "                    sw_list = pos_filter(sw_list, sub_word)\n",
    "                    for k_word, k_sim in sw_list:\n",
    "                        victim, sim_value = k_word, float(k_sim)\n",
    "                        if sim_value < 1 - d_v:\n",
    "                            continue\n",
    "                        sim_value = 1 - sim_value\n",
    "                        victim_index = [ind for ind, x in enumerate(doc) if ((x == victim) and (ind not in adv_solu.keys()))][0]\n",
    "                        if victim_index not in already:\n",
    "                            base4flag = 1\n",
    "                            att_words.append(sub_word)\n",
    "                            break\n",
    "                    if base4flag == 0:\n",
    "                        base4iter += 1    \n",
    "            else:\n",
    "                victim_index = victims[i]\n",
    "                victim = doc[victim_index]\n",
    "                victim_pos = nltk.pos_tag([victim])[0][1]\n",
    "                for p in wordq:\n",
    "                    if victim_pos in p:\n",
    "                        break\n",
    "\n",
    "                if Att_algo == 'baseline_1':\n",
    "                    sub_word = random.choice(np.array(new_we['word'].tolist()))\n",
    "                    while (nltk.pos_tag([sub_word])[0][1] not in p) or (sub_word == victim):\n",
    "                        sub_word = random.choice(np.array(new_we['word'].tolist()))\n",
    "                    \n",
    "                    sim_value = (1 - pdist([np.array(new_we[new_we['word'].isin([sub_word])]['word_embedding'].tolist()[0]), \n",
    "                                            np.array(new_we[new_we['word'].isin([victim])]['word_embedding'].tolist()[0])],\n",
    "                                            'cosine'))[0]\n",
    "                    sim_value = 1 - sim_value\n",
    "\n",
    "                elif Att_algo == 'baseline_2':\n",
    "                    target_list = new_we[new_we['word'].isin([victim])]['similar_words'].tolist()[0]\n",
    "                    for tar in target_list:\n",
    "                        if nltk.pos_tag([tar[0]])[0][1] not in p:\n",
    "                            continue\n",
    "                    sub_word, sim_value = tar\n",
    "                    sim_value = float(sim_value)\n",
    "\n",
    "                elif Att_algo == 'baseline_3':\n",
    "                    target_index = attack_words[base3iter]\n",
    "                    sub_word = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "\n",
    "                    while (nltk.pos_tag([sub_word])[0][1] not in p) or (sub_word == victim) or (sub_word in att_words):\n",
    "                        base3iter += 1\n",
    "                        target_index = attack_words[base3iter]\n",
    "                        sub_word = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "\n",
    "                    sim_value = word_embedding_model.similarity(sub_word, victim)\n",
    "                    sim_value = 1 - sim_value\n",
    "                    att_words.append(sub_word)\n",
    "\n",
    "            word_candidate = (victim, sub_word, 0, victim_index, float(sim_value))     \n",
    "            pos = victim_index\n",
    "            \n",
    "            ori_sentence, adv_sentence = merge_sentence(pos, doc, word_candidate)\n",
    "             # Calculate ASSD\n",
    "            ori_sentence_em = bc.encode([ori_sentence])[0]\n",
    "            adv_sentence_em = bc.encode([adv_sentence])[0]\n",
    "            sentence_dis += pdist([ori_sentence_em, adv_sentence_em], 'cosine')\n",
    "            \n",
    "            doc[victim_index] = word_candidate[1]  # modificate \n",
    "            cur_change += 1\n",
    "            adv_solu[(victim_index, word_candidate[0])] = word_candidate[1]  # save attack path\n",
    "            already.append(word_candidate[3])\n",
    "            word_dis += word_candidate[-1]\n",
    "    else:  # EvaLDA\n",
    "        while (cur_change < iterations):\n",
    "            word_cos_max = float('-inf')\n",
    "            for index, word in enumerate(list(set(doc))):  # find victim word\n",
    "                if word in vic_visited:\n",
    "                    continue\n",
    "                word_index = voc[word]  # victim word's index\n",
    "                if word not in new['word'].values or word_counts[word] < 1:\n",
    "                    continue\n",
    "                similar_words, ori_phi = get_similar_words(word_index, word, Nkv, new)\n",
    "                if similar_words == []:  # wordnet may don't have similar words\n",
    "                    continue          \n",
    "                for (sim_word, sim_value) in similar_words:\n",
    "                    if (sim_word not in voc) or float(sim_value) < 1 - d_v:  # distance budget\n",
    "                        continue\n",
    "                    sim_index = voc[sim_word]\n",
    "                    new_phi = Nkv[:, sim_index] / sum(Nkv[:, sim_index])\n",
    "                    if t >= (iterations - cur_change):  # calculate the real modification budget\n",
    "                        tmp = iterations - cur_change\n",
    "                    else:\n",
    "                        tmp = t\n",
    "                    real_t = min(tmp, word_counts[word])\n",
    "                    tem = contributions + (new_phi[target] - word_contribution[word_index]) * real_t\n",
    "                    score = tem ** int(Att_algo[-1]) - contributions ** int(Att_algo[-1])\n",
    "                    if score > word_cos_max: # update word_candidate\n",
    "                        word_cos_max = score\n",
    "                        sim_value = (1 - float(sim_value)) \n",
    "                        word_candidate = (word, sim_word, score, index, float(sim_value), real_t)\n",
    "                    elif score == word_cos_max:\n",
    "                        if word_candidate and word_candidate[-2] > (1 - float(sim_value)):\n",
    "                            sim_value = (1 - float(sim_value)) \n",
    "                            word_cos_max = score\n",
    "                            word_candidate = (word, sim_word, score, index, float(sim_value), real_t)   \n",
    "            \n",
    "            pos_candidate = []\n",
    "            for ind, x in enumerate(doc):\n",
    "                if (x == word_candidate[0]) and (ind not in already):\n",
    "                    pos_candidate.append(ind)\n",
    "            p_count = 0  # current modification of the unique word\n",
    "            for pos in pos_candidate:\n",
    "                ori_sentence, adv_sentence = merge_sentence(pos, doc, word_candidate)\n",
    "                ori_sentence_em = bc.encode([ori_sentence])[0]\n",
    "                adv_sentence_em = bc.encode([adv_sentence])[0]\n",
    "                sentence_dis += pdist([ori_sentence_em, adv_sentence_em], 'cosine')  # ASSD\n",
    "\n",
    "                doc[pos] = word_candidate[1]  # modificate the victim document\n",
    "                adv_solu[(pos, word_candidate[0])] = word_candidate[1]  # save attack path\n",
    "                already.append(pos)\n",
    "                word_dis += word_candidate[-2]\n",
    "                word_counts[word_candidate[0]] -= real_t\n",
    "                word_counts[word_candidate[1]] += real_t\n",
    "                # update contributions\n",
    "                contributions += (Nkv[:, voc[word_candidate[1]]] / sum(Nkv[:, voc[word_candidate[1]]]))[target] \\\n",
    "                                 - word_contribution[voc[word_candidate[0]]]\n",
    "                cur_change += real_t\n",
    "                p_count += 1\n",
    "                if p_count == word_candidate[-1]:\n",
    "                    break;\n",
    "                if cur_change == iterations: \n",
    "                    break\n",
    "            vic_visited.append(word_candidate[0])\n",
    "\n",
    "    word_dis /= cur_change  # take average\n",
    "    sentence_dis /= cur_change \n",
    "    return adv_solu, nd, doc, word_dis, sentence_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01  # modification threshold\n",
    "k = 1 # modify the unique word in k positions in each iteration\n",
    "test_list = list(range(0, submissions.index.size))\n",
    "random.shuffle(test_list) # you should record this test_list to ensure the same test samples\n",
    "test_iterations = 50 # run 50 samples\n",
    "result = collections.defaultdict(list)\n",
    "target_list = {0: 19, 1: 14, 2: 9, 3: 4} # original topic rankings, counting from 0.\n",
    "total_result = []\n",
    "total_path = []\n",
    "for t in range(4):  # different rank_cal\n",
    "    result = collections.defaultdict(list)\n",
    "    Path = collections.defaultdict(list)\n",
    "    for i in range(test_iterations):\n",
    "        sub_sample = submissions.iloc[test_list[i]]['paper_text_tokens']\n",
    "\n",
    "        length = len(sub_sample)\n",
    "        del_list = []\n",
    "        for index, w in enumerate(sub_sample):\n",
    "            if w not in voc or w not in word_embedding_model:\n",
    "                del_list.append(index)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        sub_sample = [sub_sample[i] for i in range(0, len(sub_sample),\n",
    "            1) if i not in del_list]\n",
    "        word_index_list = []\n",
    "        for word in sub_sample:\n",
    "            word_index_list.append(voc[word]) \n",
    "        if word_index_list == []:\n",
    "            continue\n",
    "        '''\n",
    "        -----------------Before attack-----------------\n",
    "        '''\n",
    "        n_d_sub = infer(model.nzw_, word_index_list)\n",
    "        d_t_sub = n_d_sub / sum(n_d_sub)\n",
    "\n",
    "        output_new = {}\n",
    "        for index, i in enumerate(d_t_sub):\n",
    "            output_new[index] = i\n",
    "\n",
    "        output_new = sorted(output_new.items(), key=lambda x:x[1], reverse=True)\n",
    "        common = []\n",
    "        for j in output_new:\n",
    "            if j[1] != 0:\n",
    "                common.append(list(j))\n",
    "        target = int(np.array(common)[:, 0][target_list[t]])  # target rank\n",
    "        str_tmp = 'Common_' + str(t + 1)\n",
    "        for index, i in enumerate(output_new):\n",
    "            if i[0] == target:\n",
    "                result[str_tmp].append([index+1, i[0], i[1]])\n",
    "        '''\n",
    "        -----------------EvaLDA: level 4-----------------\n",
    "        '''\n",
    "        str_tmp = 'level_4_' + str(t + 1)\n",
    "        adv_path_2, new_nd, new_doc, sim_w, sim_s = \\\n",
    "            seak_for_word(copy.deepcopy(sub_sample), target,\n",
    "                        n_d_sub, model.nzw_, length, 'level_4',\n",
    "                        k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word])\n",
    "        n_d_new_2 = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new_2, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path_2])\n",
    "        '''\n",
    "        -----------------Baseline 1-----------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_1_' + str(t + 1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = \\\n",
    "            seak_for_word(copy.deepcopy(sub_sample), target,\n",
    "                        n_d_sub, model.nzw_, length, 'baseline_1',\n",
    "                        k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word])\n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "        '''\n",
    "        -----------------Baseline 2-----------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_2_' + str(t + 1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = \\\n",
    "            seak_for_word(copy.deepcopy(sub_sample), target,\n",
    "                        n_d_sub, model.nzw_, length, 'baseline_2',\n",
    "                        k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word])\n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "        '''\n",
    "        -----------------Baseline 3-----------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_3_' + str(t + 1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = \\\n",
    "            seak_for_word(copy.deepcopy(sub_sample), target,\n",
    "                        n_d_sub, model.nzw_, length, 'baseline_3',\n",
    "                        k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word])\n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str].append([adv_path])\n",
    "        '''\n",
    "        -----------------Baseline 4-----------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_4_' + str(t + 1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = \\\n",
    "            seak_for_word(copy.deepcopy(sub_sample), target,\n",
    "                        n_d_sub, model.nzw_, length, 'baseline_4',\n",
    "                        k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word])\n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "        print(result)\n",
    "    total_path.append(Path)\n",
    "    total_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_ans(result):\n",
    "    res = [[0, 0, 0, 0], [0,0, 0, 0], [0, 0,0, 0], [0,0, 0, 0], [0,0, 0, 0], [0, 0,0, 0]]\n",
    "    for index, i in enumerate(result):\n",
    "        for jindex, j in enumerate(result[i]):\n",
    "            res[index][0] += j[0]\n",
    "            res[index][1] += j[2]\n",
    "            res[index][2] += j[-2]\n",
    "            res[index][3] += j[-1]\n",
    "    return np.array(res)/len_vas\n",
    "print(res_ans(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(thre_result):\n",
    "    for index, i in enumerate(thre_result):\n",
    "        print(i)\n",
    "        a = np.array(thre_result[i][:])[:,0]\n",
    "        print('95% confidence interval：rank\\n', [np.mean(a) - 1.96 * np.std(a) / np.sqrt(len(a)), np.mean(a) + 1.96 * np.std(a) / np.sqrt(len(a))])\n",
    "        a = np.array(thre_result[i][:])[:,2]\n",
    "        print('95% confidence interval：scores\\n', [np.mean(a) - 1.96 * np.std(a) / np.sqrt(len(a)), np.mean(a) + 1.96 * np.std(a) / np.sqrt(len(a))])\n",
    "        if index == 0:\n",
    "            continue\n",
    "        a = np.array(thre_result[i][:])[:,3]\n",
    "        print('95% confidence interval：dis_word\\n', [np.mean(a) - 1.96 * np.std(a) / np.sqrt(len(a)), np.mean(a) + 1.96 * np.std(a) / np.sqrt(len(a))])\n",
    "        a = np.array(thre_result[i][:])[:,4]\n",
    "        print('95% confidence interval：dis_sentence\\n', [np.mean(a) - 1.96 * np.std(a) / np.sqrt(len(a)), np.mean(a) + 1.96 * np.std(a) / np.sqrt(len(a))])\n",
    "confidence(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
