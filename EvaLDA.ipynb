{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import nltk\n",
    "import numba\n",
    "import brisk\n",
    "import random\n",
    "import pickle\n",
    "import bisect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "import collections\n",
    "from numba import jit\n",
    "from scipy.spatial.distance import pdist\n",
    "import gensim\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet as wn\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 120\n",
    "rng = np.random.mtrand._rand\n",
    "rands = rng.rand(1024 ** 2 // 8)\n",
    "\n",
    "# Load the model\n",
    "path_dir = '../dataset/'\n",
    "datasets = 'nips'\n",
    "with open(path_dir + datasets + '/gibbs_lda_model.pickle','rb') as file:\n",
    "    model = pickle.loads(file.read())\n",
    "\n",
    "# Load the vocab\n",
    "path = path_dir + datasets + '/lda_data_3len/'\n",
    "vocab = lda.datasets.load_reuters_vocab(path)\n",
    "from collections import defaultdict\n",
    "voc = defaultdict()\n",
    "for index, i in enumerate(vocab):\n",
    "    word = i.replace('\\n', '')\n",
    "    if not word:\n",
    "        continue\n",
    "    voc[word] = index\n",
    "\n",
    "# Load test set and candidate set\n",
    "we = pd.read_csv(path_dir + datasets +  '/sy3sjj.csv')\n",
    "we['similar_words'] = we['similar_words'].map(lambda x: eval(x))\n",
    "\n",
    "def eval_string(x):\n",
    "    x = x.replace('[','').replace(']','').replace(',','')\n",
    "    x = x.strip().split()\n",
    "    x = [float(i) for i in x]\n",
    "    return x\n",
    "we['word_embedding'] = we['word_embedding'].map(lambda x:eval_string(x))\n",
    "\n",
    "f_s = open(path_dir + datasets + '/submissions.csv')\n",
    "submissions = pd.read_csv(f_s)\n",
    "submissions['paper_text_tokens'] = submissions['paper_text_tokens'].map(lambda x: eval(x))\n",
    "\n",
    "# Load word embedding (it will cost a lot of time)\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(path_dir + 'wikien.vec')\n",
    "\n",
    "# Load bert\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evals(x):\n",
    "    if x == '<class \\'list\\'>':\n",
    "        return []\n",
    "    else:\n",
    "        return eval(x)\n",
    "we['similar_mixed'] = we['similar_mixed'].map(lambda x: evals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True,nogil=True,parallel=True)\n",
    "def infer(nzw, x_index, n_iter=500, m_alpha=0.1, m_eta=0.01):\n",
    "    '''\n",
    "    infer the new documents \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array-like, shape(, n_features)\n",
    "        Testing vector, where n_features is the number of features.\n",
    "    '''\n",
    "    global n_topics\n",
    "    n_rand = 131072 # a large number\n",
    "    alpha = np.repeat(m_alpha, n_topics).astype(np.float64)\n",
    "    eta = np.repeat(m_eta, 27176).astype(np.float64) # 27176 : len(voc)\n",
    "    eta_sum = np.sum(eta) \n",
    "\n",
    "    # initialize\n",
    "    nz = np.zeros(n_topics)\n",
    "    ZS = np.empty(len(x_index), np.int32)\n",
    "    dist_sum = np.zeros(n_topics)\n",
    "    for index, w in enumerate(x_index):\n",
    "        z_new = index % n_topics\n",
    "        ZS[index] = z_new\n",
    "        nzw[z_new, w] += 1\n",
    "        nz[z_new] += 1\n",
    "    # sampling\n",
    "    for it in range(n_iter):\n",
    "        for index, w in enumerate(x_index):\n",
    "            z = ZS[index]\n",
    "            nzw[z, w] -= 1\n",
    "            nz[z] -= 1\n",
    "            dist_cum = 0\n",
    "            for k in range(n_topics):\n",
    "                dist_cum += (nzw[k, w] + eta[w]) / (nz[k] + eta_sum) * (nz[k] + alpha[k])\n",
    "                dist_sum[k] = dist_cum\n",
    "\n",
    "            r = rands[index % n_rand] * dist_cum \n",
    "            z_new = brisk.bisect_left(dist_sum, r)\n",
    "            ZS[index] = z_new\n",
    "            nzw[z_new, w] += 1\n",
    "            nz[z_new] += 1\n",
    "\n",
    "    return nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diff(word_count, voc, Nkv, target):\n",
    "    diff = 0\n",
    "    for (word, count) in word_count.items():\n",
    "        wi = voc[word]\n",
    "        diff += count * (Nkv[:, wi][target]) / brisk.sum(Nkv[:, wi])\n",
    "\n",
    "    return diff\n",
    "\n",
    "def rank_cal(x, target, result, sim_w, sim_s, flag):\n",
    "    new_x = x / sum(x)\n",
    "    output = {}\n",
    "    for index, k in enumerate(new_x):\n",
    "        output[index] = k\n",
    "\n",
    "    output = sorted(output.items(), key = lambda x:x[1], reverse = True)\n",
    "    for index, k in enumerate(output):\n",
    "        if k[0] != target and k[1] == 0:\n",
    "            result[flag].append([index+1, k[0], k[1], sim_w, sim_s])\n",
    "            break\n",
    "        if k[0] == target:\n",
    "            result[flag].append([index+1, k[0], k[1], sim_w, sim_s])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech screening\n",
    "wordq = [['JJ', 'JJR', 'JJS'], ['NN', 'NNS', 'NNP', 'NNPS'], ['PRP', 'PRP$'], ['RB', 'RBR', 'RBS'],\n",
    "         ['VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n",
    "        \n",
    "def pos_filter(x, word):\n",
    "    if x[0][0] == word:\n",
    "        del x[0]\n",
    "    pos_list = nltk.pos_tag(np.array(x)[:, 0].tolist())\n",
    "    pos_list = np.array(pos_list)[:, 1]\n",
    "    \n",
    "    target = nltk.pos_tag([word])\n",
    "    target_list = []\n",
    "    for i in wordq:\n",
    "        if target[0][1] in i:\n",
    "            target_list = i\n",
    "\n",
    "    index = [k for k in range(len(np.array(pos_list))) if (pos_list[k] in target_list)]\n",
    "    res = [tuple(p) for p in np.array(x)[index]]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(word_index, word, Nkv, new):\n",
    "    em_word_index = int(list(new[new['word'].isin([word])].index.values)[0])\n",
    "    ori_phi = Nkv[:, word_index] / sum(Nkv[:, word_index])\n",
    "    similar_words = new.loc[em_word_index, 'similar_mixed']\n",
    "    similar_words = pos_filter(similar_words, word)\n",
    "    return similar_words, ori_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeakForWord(doc, target, nd, Nkv, length, Attack_algo, t, alpha = 0.005):\n",
    "    '''\n",
    "    Input\n",
    "        doc: the victim submission\n",
    "        target: the target topic\n",
    "        nd: the predicted topic sampling count of the victim submission\n",
    "        Nkv: the training topic-word sampling count of whole voc\n",
    "        length: the length of original victim submission\n",
    "        Attack_algo: attack algorithm, different level parameter\n",
    "        t: the unique words substitution parameter\n",
    "        alpha: modification budget\n",
    "    Output\n",
    "        attack path, adversarial topic sampling count estimation, adversarial submission\n",
    "    '''\n",
    "    global voc\n",
    "    global path\n",
    "    # Get solution space\n",
    "    solution_space = []\n",
    "    for i in sub_sample:\n",
    "        if i in we['word'].values and i not in solution_space:\n",
    "            solution_space.append(i)\n",
    "\n",
    "    new_we = we[we['word'].isin(solution_space)]\n",
    "    new = copy.deepcopy(new_we)\n",
    "    iterations = int(length * alpha)\n",
    "    nd = nd.astype('float64')\n",
    "    word_dis, sentence_dis = 0.0, 0.0 # word distortion, sentence distortion\n",
    "    adv_solu = {} # attack path\n",
    "    already = []\n",
    "    vic_visited = []\n",
    "    word_counts = collections.Counter(doc) # word counts\n",
    "    d_v = 0.4 # word embedding loss threshold\n",
    "    cur_change = 0 # current modification count\n",
    "    \n",
    "    # word-phi dict\n",
    "    word_contribution = collections.defaultdict(list)\n",
    "    contributions = 0.0\n",
    "    for index, word in enumerate(list(set(doc))):\n",
    "        word_index = voc[word]\n",
    "        phi = Nkv[:, word_index] / sum(Nkv[:, word_index])\n",
    "        word_contribution[word_index] = phi[target]\n",
    "        contributions += (word_contribution[word_index] * word_counts[word])\n",
    "    \n",
    "    if 'baseline' in Attack_algo:\n",
    "        if Attack_algo != 'baseline_4':\n",
    "            victims = random.sample(range(len(doc)), iterations)\n",
    "        if Attack_algo == 'baseline_3' or Attack_algo == 'baseline_4':\n",
    "            attack_words = (-model.topic_word_[target]).argsort()[:]\n",
    "            base3iter = 0\n",
    "            base4iter = 0\n",
    "            att_words = []\n",
    "        for i in range(iterations):\n",
    "            if Attack_algo == 'baseline_4':\n",
    "                base4flag = 0\n",
    "                while base4flag == 0:\n",
    "                    target_index = attack_words[base4iter]\n",
    "                    target = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "                    if target in att_words:\n",
    "                        base4iter += 1\n",
    "                        continue\n",
    "                    # get the most similar words in doc\n",
    "                    sw_list = word_embedding_model.most_similar_to_given_top(target, list(set(doc)), topk=len(list(set(doc))))\n",
    "                    # pos filter\n",
    "                    sw_list = pos_filter(sw_list, target)\n",
    "                    for k_word, k_sim in sw_list:\n",
    "                        victim, sim_value = k_word, float(k_sim)\n",
    "                        if sim_value < 1 - d_v:\n",
    "                            continue\n",
    "                        sim_value = 1 - sim_value\n",
    "                        victim_index = [ind for ind, x in enumerate(doc) if ((x == victim) and (ind not in adv_solu.keys()))][0]\n",
    "                        if victim_index not in already:\n",
    "                            base4flag = 1\n",
    "                            att_words.append(target)\n",
    "                            break\n",
    "                    if base4flag == 0:\n",
    "                        base4iter += 1\n",
    "                    \n",
    "            else:\n",
    "                victim_index = victims[i]\n",
    "                victim = doc[victim_index]\n",
    "                victim_pos = nltk.pos_tag([victim])[0][1]\n",
    "                for p in wordq:\n",
    "                    if victim_pos in p:\n",
    "                        break\n",
    "\n",
    "                if Attack_algo == 'baseline_1':\n",
    "                    target = random.choice(np.array(new_we['word'].tolist()))\n",
    "                    while (nltk.pos_tag([target])[0][1] not in p) or (target == victim):\n",
    "                        target = random.choice(np.array(new_we['word'].tolist()))\n",
    "                    \n",
    "                    sim_value = (1 - pdist([np.array(new_we[new_we['word'].isin([target])]['word_embedding'].tolist()[0]), \n",
    "                                            np.array(new_we[new_we['word'].isin([victim])]['word_embedding'].tolist()[0])], 'cosine'))[0]\n",
    "                    sim_value = 1 - sim_value\n",
    "\n",
    "                elif Attack_algo == 'baseline_2':\n",
    "                    target_list = new_we[new_we['word'].isin([victim])]['similar_words'].tolist()[0]\n",
    "                    for tar in target_list:\n",
    "                        if nltk.pos_tag([tar[0]])[0][1] not in p:\n",
    "                            continue\n",
    "                    target, sim_value = tar\n",
    "                    sim_value = float(sim_value)\n",
    "\n",
    "                elif Attack_algo == 'baseline_3':\n",
    "                    target_index = attack_words[base3iter]\n",
    "                    target = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "\n",
    "                    while (nltk.pos_tag([target])[0][1] not in p) or (target == victim) or (target in att_words):\n",
    "                        base3iter += 1\n",
    "                        target_index = attack_words[base3iter]\n",
    "                        target = list(voc.keys())[list(voc.values()).index(target_index)]\n",
    "\n",
    "                    sim_value = word_embedding_model.similarity(target, victim)\n",
    "                    sim_value = 1 - sim_value\n",
    "                    att_words.append(target)\n",
    "\n",
    "            word_candidate = (victim, target, 0, victim_index, float(sim_value))     \n",
    "            pos = victim_index\n",
    "             # Calculate ASSD\n",
    "            if pos >= 5 and pos <= len(doc) - 5:\n",
    "                ori_sentence = ' '.join(doc[pos - 5: pos + 5]) # combine sentence\n",
    "                adv_sentence = ' '.join(doc[pos - 5: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: pos + 5])\n",
    "            elif pos < 5:\n",
    "                ori_sentence = ' '.join(doc[:10])\n",
    "                adv_sentence = ' '.join(doc[0: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: 10])\n",
    "            else:\n",
    "                ori_sentence = ' '.join(doc[-10:])\n",
    "                adv_sentence = ' '.join(doc[-10: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1:])\n",
    "\n",
    "            ori_sentence_em = bc.encode([ori_sentence])[0]\n",
    "            adv_sentence_em = bc.encode([adv_sentence])[0]\n",
    "            sentence_dis += pdist([ori_sentence_em, adv_sentence_em], 'cosine')\n",
    "            \n",
    "            doc[victim_index] = word_candidate[1] # modificate \n",
    "            cur_change += 1\n",
    "            adv_solu[(victim_index, word_candidate[0])] = word_candidate[1] # save attack path\n",
    "            already.append(word_candidate[3])\n",
    "            word_dis += word_candidate[-1]\n",
    "    else:\n",
    "        while (cur_change < iterations):\n",
    "            word_cos_max = float('-inf')\n",
    "            for index, word in enumerate(list(set(doc))): # find victim word\n",
    "                if word in vic_visited:\n",
    "                    continue\n",
    "                word_index = voc[word] # victim word's index\n",
    "                if word not in new['word'].values or word_counts[word] < 1:\n",
    "                    continue\n",
    "                similar_words, ori_phi = get_similar_words(word_index, word, Nkv, new)\n",
    "                if similar_words == []: # word net may dont have similar words\n",
    "                    continue          \n",
    "                for (sim_word, sim_value) in similar_words:\n",
    "                    if (sim_word not in voc) or float(sim_value) < 1 - d_v: # distance budget\n",
    "                        continue\n",
    "                    sim_index = voc[sim_word]\n",
    "                    if Attack_algo == 'origin_level':\n",
    "                        new_phi = Nkv[:, sim_index] / sum(Nkv[:, sim_index])\n",
    "                        phi_diff = (new_phi - ori_phi)\n",
    "                        new_mk = nd + phi_diff\n",
    "                        new_n_d_sub = new_mk / sum(new_mk)\n",
    "                        ori_n_d_sub = nd / sum(nd)\n",
    "                        score = new_n_d_sub[target] - ori_n_d_sub[target]\n",
    "                        \n",
    "                        if t >= (iterations - cur_change):\n",
    "                            tmp = iterations - cur_change\n",
    "                        else:\n",
    "                            tmp = t\n",
    "                        real_t = min(tmp, word_counts[word])\n",
    "                    else:\n",
    "                        new_phi = Nkv[:, sim_index] / sum(Nkv[:, sim_index])\n",
    "                        \n",
    "                        if t >= (iterations - cur_change):\n",
    "                            tmp = iterations - cur_change\n",
    "                        else:\n",
    "                            tmp = t\n",
    "                        real_t = min(tmp, word_counts[word])\n",
    "\n",
    "                        tem = contributions + (new_phi[target] - word_contribution[word_index]) * real_t\n",
    "                        score = tem ** (int(Attack_algo[-1]) + 1) - contributions ** (int(Attack_algo[-1]) + 1)\n",
    "                    if score > word_cos_max:\n",
    "                        word_cos_max = score\n",
    "                        sim_value = (1 - float(sim_value)) \n",
    "                        word_candidate = (word, sim_word, score, index, float(sim_value), real_t)\n",
    "                    elif score == word_cos_max:\n",
    "                        if word_candidate and word_candidate[-2] > (1 - float(sim_value)):\n",
    "                            sim_value = (1 - float(sim_value)) \n",
    "                            word_cos_max = score\n",
    "                            word_candidate = (word, sim_word, score, index, float(sim_value), real_t)   \n",
    "            \n",
    "            pos_candidate = []\n",
    "            for ind, x in enumerate(doc):\n",
    "                if (x == word_candidate[0]) and (ind not in already):\n",
    "                    pos_candidate.append(ind)\n",
    "            p_count = 0\n",
    "            for pos in pos_candidate:\n",
    "                if pos >= 5 and pos <= len(doc) - 5:\n",
    "                    ori_sentence = ' '.join(doc[pos - 5: pos + 5])\n",
    "                    adv_sentence = ' '.join(doc[pos - 5: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: pos + 5])\n",
    "                elif pos < 5:\n",
    "                    ori_sentence = ' '.join(doc[:10])\n",
    "                    adv_sentence = ' '.join(doc[0: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1: 10])\n",
    "                else:\n",
    "                    ori_sentence = ' '.join(doc[-10:])\n",
    "                    adv_sentence = ' '.join(doc[-10: pos]) + ' ' + word_candidate[1] + ' ' + ' '.join(doc[pos + 1:])\n",
    " \n",
    "                ori_sentence_em = bc.encode([ori_sentence])[0]\n",
    "                adv_sentence_em = bc.encode([adv_sentence])[0]\n",
    "                sentence_dis += pdist([ori_sentence_em, adv_sentence_em], 'cosine') # AWD\n",
    "\n",
    "                doc[pos] = word_candidate[1] # modificate the victim document\n",
    "                adv_solu[(pos, word_candidate[0])] = word_candidate[1] # save attack path\n",
    "                already.append(pos)\n",
    "                word_dis += word_candidate[-2]\n",
    "                word_counts[word_candidate[0]] -= real_t\n",
    "                word_counts[word_candidate[1]] += real_t\n",
    "                # update contributions\n",
    "                contributions += (Nkv[:, voc[word_candidate[1]]] / sum(Nkv[:, voc[word_candidate[1]]]))[target] \\\n",
    "                                 - word_contribution[voc[word_candidate[0]]]\n",
    "                cur_change += real_t\n",
    "                p_count += 1\n",
    "                if p_count == word_candidate[-1]:\n",
    "                    break;\n",
    "                if cur_change == iterations: \n",
    "                    break\n",
    "            if Attack_algo == 'origin_level':\n",
    "                nd += (word_candidate[2])\n",
    "            vic_visited.append(word_candidate[0])\n",
    "\n",
    "    word_dis /= cur_change # take average\n",
    "    sentence_dis /= cur_change \n",
    "    return adv_solu, nd, doc, word_dis, sentence_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "alpha = 0.01\n",
    "k_num = 2 \n",
    "test_list = list(range(0, 678))\n",
    "\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "target_list = {0: 19, 1: 14, 2: 9, 3: 4}\n",
    "total_result = []\n",
    "total_path = []\n",
    "for t in range(4):\n",
    "    result = collections.defaultdict(list)\n",
    "    sim = []\n",
    "    path = []\n",
    "    path_1 = []\n",
    "    path_2 = []\n",
    "    path_2_d = []\n",
    "    path_3_d = []\n",
    "    path_4_d = []\n",
    "    path_5_d = []\n",
    "    path_6_d = []\n",
    "    Path = collections.defaultdict(list)\n",
    "    for i in range(50):\n",
    "        now = time.time()\n",
    "        sub_sample = submissions.iloc[test_list[i]]['paper_text_tokens']\n",
    "\n",
    "        length = len(sub_sample)\n",
    "        del_list = []\n",
    "        for index, w in enumerate(sub_sample):\n",
    "            if w not in voc or w not in word_embedding_model:\n",
    "                del_list.append(index)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        sub_sample = [sub_sample[i] for i in range(0, len(sub_sample), 1) if i not in del_list]\n",
    "        word_index_list = []\n",
    "        for word in sub_sample:\n",
    "            word_index_list.append(voc[word]) \n",
    "        if word_index_list == []:\n",
    "            continue\n",
    "        '''\n",
    "        -------------------------------------Common State: 未受攻击时的正常预测---------------------------------------------\n",
    "        '''\n",
    "        a = time.time()\n",
    "        n_d_sub = infer(model.nzw_, word_index_list, 500)\n",
    "        d_t_sub = n_d_sub / sum(n_d_sub)\n",
    "\n",
    "        output_new = {}\n",
    "        for index, i in enumerate(d_t_sub):\n",
    "            output_new[index] = i\n",
    "\n",
    "        output_new = sorted(output_new.items(),key = lambda x:x[1],reverse = True)\n",
    "        common = []\n",
    "        for j in output_new:\n",
    "            if j[1] != 0:\n",
    "                common.append(list(j))\n",
    "        # 固定ranking\n",
    "        target = int(np.array(common)[:,0][target_list[t]])\n",
    "        str_tmp = 'Common_'+str(t+1)\n",
    "        for index, i in enumerate(output_new):\n",
    "            if i[0] == target:\n",
    "                result[str_tmp].append([index+1, i[0], i[1]])\n",
    "        '''\n",
    "        -------------------------------------Ours Attack:      level_4----------------------------------------------------\n",
    "        '''\n",
    "        for k in range(1, k_num):\n",
    "            str_tmp = 'level_4_' + str(t+1)\n",
    "            adv_path_2, new_nd, new_doc, sim_w, sim_s = SeakForWord(copy.deepcopy(sub_sample), target, n_d_sub, model.nzw_, \n",
    "                                                             length, 'Ours_level_3', k, alpha=alpha)\n",
    "            word_index_list = []\n",
    "            for word in new_doc:\n",
    "                word_index_list.append(voc[word]) \n",
    "            n_d_new_2 = infer(model.nzw_, word_index_list)\n",
    "            result = rank_cal(n_d_new_2, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "            Path[str_tmp].append([adv_path_2])\n",
    "        '''\n",
    "        ------------------------------------------------Baseline 1---------------------------------------------------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_1_' + str(t+1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = SeakForWord(copy.deepcopy(sub_sample), target, n_d_sub, model.nzw_, length, \n",
    "                                                       'baseline_1', k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word]) \n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "        '''\n",
    "        ------------------------------------------------Baseline 2---------------------------------------------------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_2_' + str(t+1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = SeakForWord(copy.deepcopy(sub_sample), target, n_d_sub, model.nzw_, length,\n",
    "                                                       'baseline_2', k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word]) \n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "        '''\n",
    "        ------------------------------------------------Baseline 3---------------------------------------------------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_3_' + str(t+1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = SeakForWord(copy.deepcopy(sub_sample), target, n_d_sub, model.nzw_, length,\n",
    "                                                       'baseline_3', k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word]) \n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str].append([adv_path])\n",
    "        '''\n",
    "        ------------------------------------------------Baseline 4---------------------------------------------------------\n",
    "        '''\n",
    "        str_tmp = 'baseline_4_' + str(t+1)\n",
    "        adv_path, new_nd, new_doc, sim_w, sim_s = SeakForWord(copy.deepcopy(sub_sample), target, n_d_sub, model.nzw_, length,\n",
    "                                                       'baseline_4', k, alpha=alpha)\n",
    "        word_index_list = []\n",
    "        for word in new_doc:\n",
    "            word_index_list.append(voc[word]) \n",
    "        n_d_new = infer(model.nzw_, word_index_list)\n",
    "        result = rank_cal(n_d_new, target, result, sim_w, sim_s, str_tmp)\n",
    "\n",
    "        Path[str_tmp].append([adv_path])\n",
    "\n",
    "\n",
    "        print('time', time.time() - now)\n",
    "        print(result)\n",
    "    total_path.append(Path)\n",
    "    total_result.append(result)\n",
    "\n",
    "print('total time', time.time() - now)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
